# MicroBERT MLM Pre-training

A lightweight BERT implementation for Masked Language Modeling (MLM) pre-training with support for multiple datasets and streaming capabilities.

## Features

- ğŸš€ **Lightweight BERT**: Small, efficient BERT implementation
- ğŸ“Š **Multiple Datasets**: Support for IMDB and Hugging Face datasets
- ğŸ’¾ **Streaming Support**: Memory-efficient data loading with local caching
- ğŸ¯ **MLM Pre-training**: Full Masked Language Modeling implementation
- ğŸ“ˆ **Training Visualization**: Built-in plotting and monitoring
- ğŸ”§ **Flexible Configuration**: Easy model parameter tuning

## Installation

### 1. Clone the repository
```bash
git clone <repository-url>
cd microbert
```

### 2. Create a virtual environment (recommended)
```bash
# Using conda
conda create -n microbert python=3.10
conda activate microbert

# Or using venv
python -m venv microbert_env
source microbert_env/bin/activate  # On Windows: microbert_env\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Install the package in development mode
```bash
pip install -e .
```

## Quick Start

### ğŸ¯ **é€‰æ‹©é€‚åˆä½ çš„è®­ç»ƒè„šæœ¬**

æ ¹æ®ä½ çš„éœ€æ±‚å’Œç¡¬ä»¶é…ç½®ï¼Œé€‰æ‹©ä»¥ä¸‹å››ä¸ªç‰ˆæœ¬ä¹‹ä¸€ï¼š

#### **v1: å¿«é€Ÿå…¥é—¨ (IMDBæ•°æ®é›†)**
```bash
# ä½¿ç”¨IMDBæ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
python mlm_pretrain_v1.py
```
- **é€‚ç”¨åœºæ™¯**: å­¦ä¹ ã€æµ‹è¯•ã€å¿«é€ŸéªŒè¯
- **æ•°æ®é›†**: IMDBç”µå½±è¯„è®º (~25Kæ ·æœ¬)
- **æ¨¡å‹**: å°æ¨¡å‹ (2å±‚, 2å¤´, 4ç»´åµŒå…¥)
- **è®­ç»ƒæ—¶é—´**: ~5åˆ†é’Ÿ
- **å†…å­˜éœ€æ±‚**: ä½

#### **v2: æ ‡å‡†è®­ç»ƒ (Hugging Faceæ•°æ®é›†)**
```bash
# ä½¿ç”¨Hugging Faceå¤§æ•°æ®é›† (é»˜è®¤500Kæ ·æœ¬)
python mlm_pretrain_v2.py hf

# æŒ‡å®šæ•°æ®å¤§å° (5Mæ ·æœ¬)
python mlm_pretrain_v2.py hf true 5M

# æŒ‡å®šæ•°æ®å¤§å° (50Mæ ·æœ¬)
python mlm_pretrain_v2.py hf false 50M

# æˆ–ä½¿ç”¨IMDBæ•°æ®é›†
python mlm_pretrain_v2.py imdb
```
- **é€‚ç”¨åœºæ™¯**: æ ‡å‡†è®­ç»ƒã€ä¸­ç­‰è§„æ¨¡æ•°æ®é›†
- **æ•°æ®é›†**: Hugging Faceæ•°æ®é›† (å¯é…ç½®å¤§å°: 500K-500Mæ ·æœ¬) æˆ– IMDB
- **æ¨¡å‹**: ä¸­ç­‰æ¨¡å‹ (4å±‚, 4å¤´, 8ç»´åµŒå…¥) æˆ–å°æ¨¡å‹
- **è®­ç»ƒæ—¶é—´**: ~30åˆ†é’Ÿ (500K) / ~2å°æ—¶ (5M) / ~20å°æ—¶ (50M)
- **å†…å­˜éœ€æ±‚**: ä¸­ç­‰

#### **v3: å¤šGPUè®­ç»ƒ (H200 8å¡)**
```bash
# ä½¿ç”¨é¢„é…ç½®è„šæœ¬ (æ¨è)
python multi_gpu_configs.py generate h200_8gpu_standard
./train_h200_8gpu_standard.sh

# æˆ–ç›´æ¥ä½¿ç”¨torchrun (é»˜è®¤500Kæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v3.py \
    --dataset hf \
    --batch-size 32 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 500k
```
- **é€‚ç”¨åœºæ™¯**: å¤šGPUè®­ç»ƒã€å¤§è§„æ¨¡æ•°æ®é›†
- **æ•°æ®é›†**: Hugging Faceæ•°æ®é›† (å¯é…ç½®å¤§å°: 500K-50Mæ ·æœ¬) æˆ– IMDB
- **æ¨¡å‹**: ä¸­ç­‰æ¨¡å‹ (6å±‚, 8å¤´, 16ç»´åµŒå…¥) æˆ–å°æ¨¡å‹
- **è®­ç»ƒæ—¶é—´**: ~15åˆ†é’Ÿ (500K) / ~1.3å°æ—¶ (5M) / ~13å°æ—¶ (50M)
- **å†…å­˜éœ€æ±‚**: ä¸­ç­‰
- **GPUè¦æ±‚**: 8å¡H200æˆ–ç±»ä¼¼é…ç½®

#### **v4: 24GBå†…å­˜ä¼˜åŒ–è®­ç»ƒ (H200/A10å…¼å®¹)**
```bash
# ä½¿ç”¨é¢„é…ç½®è„šæœ¬ (æ¨è)
./train_h200_8gpu_v4.sh

# æˆ–ç›´æ¥ä½¿ç”¨torchrun (24GBå†…å­˜ä¼˜åŒ–)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 10M
```
- **é€‚ç”¨åœºæ™¯**: 24GB GPUå†…å­˜ä¼˜åŒ–è®­ç»ƒ (H200/A10å…¼å®¹)
- **æ•°æ®é›†**: Hugging Faceæ•°æ®é›† (å¯é…ç½®å¤§å°: 500K-50Mæ ·æœ¬) æˆ– IMDB
- **æ¨¡å‹**: åŠ¨æ€é…ç½® (æ ¹æ®GPUå†…å­˜è‡ªåŠ¨è°ƒæ•´)
  - **å¤§æ¨¡å‹** (100GB+ GPU): 4å±‚, 8å¤´, 128ç»´åµŒå…¥, batch_size=16
  - **ä¸­ç­‰æ¨¡å‹** (40GB+ GPU): 6å±‚, 8å¤´, 128ç»´åµŒå…¥, batch_size=32
  - **å°æ¨¡å‹** (24GB GPU): 4å±‚, 8å¤´, 128ç»´åµŒå…¥, batch_size=8
- **è®­ç»ƒæ—¶é—´**: ~15åˆ†é’Ÿ (10Mæ ·æœ¬)
- **å†…å­˜éœ€æ±‚**: éå¸¸ä¿å®ˆé…ç½®ï¼Œç¡®ä¿å•å¡å†…å­˜ä½¿ç”¨ä¸è¶…è¿‡24GB
- **GPUè¦æ±‚**: 24GB+ GPU (H200, A10, RTX 4090ç­‰)

## Project Structure

```
microbert/
â”œâ”€â”€ microbert/                    # Core package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                 # BERT model implementation
â”‚   â”œâ”€â”€ tokenizer.py             # Word-level tokenizer
â”‚   â””â”€â”€ utils.py                 # Utility functions
â”œâ”€â”€ mlm_pretrain_v1.py           # v1: IMDB-only MLM training (å¿«é€Ÿå…¥é—¨)
â”œâ”€â”€ mlm_pretrain_v2.py           # v2: Standard MLM training with dataset options (æ ‡å‡†è®­ç»ƒ)
â”œâ”€â”€ mlm_pretrain_v3.py           # v3: Multi-GPU MLM training (å¤šGPUè®­ç»ƒ)
â”œâ”€â”€ mlm_pretrain_v4.py           # v4: 24GB memory optimized MLM training (24GBå†…å­˜ä¼˜åŒ–)
â”œâ”€â”€ multi_gpu_configs.py         # Multi-GPU training configurations
â”œâ”€â”€ run_multi_gpu_training.sh    # Multi-GPU training launcher
â”œâ”€â”€ train_h200_8gpu_standard.sh  # H200 8-GPU standard training script
â”œâ”€â”€ train_h200_8gpu_v4.sh        # H200 8-GPU v4 optimized training script
â”œâ”€â”€ train_24gb_optimized.sh      # 24GB optimized training script
â”œâ”€â”€ model_config_comparison.py   # Model configuration comparison tool
â”œâ”€â”€ test_streaming.py            # Test streaming functionality
â”œâ”€â”€ test_cache.py                # Test caching functionality
â”œâ”€â”€ cache_manager.py             # Cache management utility
â”œâ”€â”€ demo_caching.py              # Caching demo
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ requirements-minimal.txt     # Minimal dependencies
â”œâ”€â”€ setup.py                     # Package setup
â”œâ”€â”€ MULTI_GPU_USAGE.md           # Multi-GPU training guide
â”œâ”€â”€ DATASET_OPTIONS.md           # Dataset options documentation
â”œâ”€â”€ STREAMING_GUIDE.md           # Streaming and caching guide
â”œâ”€â”€ VERSION_COMPARISON.md        # Version comparison guide
â””â”€â”€ README.md                    # This file
```

## Usage Examples

### ğŸ¯ **v1: å¿«é€Ÿå…¥é—¨è®­ç»ƒ (IMDBæ•°æ®é›†)**

```bash
# ä½¿ç”¨IMDBæ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒ
python mlm_pretrain_v1.py
```

**ç‰¹ç‚¹:**
- ä½¿ç”¨25K IMDBç”µå½±è¯„è®º
- å°æ¨¡å‹: 2å±‚, 2å¤´, 4ç»´åµŒå…¥
- å¿«é€Ÿè®­ç»ƒ (~5åˆ†é’Ÿ)
- é€‚åˆå­¦ä¹ å’Œæµ‹è¯•

### ğŸš€ **v2: æ ‡å‡†è®­ç»ƒ (Hugging Faceæ•°æ®é›†)**

```bash
# ä½¿ç”¨Hugging Faceå¤§æ•°æ®é›† (æµå¼æ¨¡å¼)
python mlm_pretrain_v2.py hf

# ä½¿ç”¨Hugging Faceå¤§æ•°æ®é›† (æœ¬åœ°ä¸‹è½½æ¨¡å¼)
python mlm_pretrain_v2.py hf false

# ä½¿ç”¨IMDBæ•°æ®é›†
python mlm_pretrain_v2.py imdb
```

**ç‰¹ç‚¹:**
- æ”¯æŒå¤šç§æ•°æ®é›†: wikitext, wikipedia, openwebtextç­‰
- ä¸­ç­‰æ¨¡å‹: 4å±‚, 4å¤´, 8ç»´åµŒå…¥ (HF) æˆ– 2å±‚, 2å¤´, 4ç»´åµŒå…¥ (IMDB)
- è‡ªåŠ¨ç¼“å­˜å’Œæµå¼å¤„ç†
- è®­ç»ƒæ—¶é—´: ~30åˆ†é’Ÿ (HF) / ~5åˆ†é’Ÿ (IMDB)

### âš¡ **v3: å¤šGPUè®­ç»ƒ (H200 8å¡)**

#### **æ–¹æ³•1: ä½¿ç”¨é¢„é…ç½®è„šæœ¬**
```bash
# æŸ¥çœ‹å¯ç”¨é…ç½®
python multi_gpu_configs.py list

# ç”ŸæˆH200 8-GPUè®­ç»ƒè„šæœ¬
python multi_gpu_configs.py generate h200_8gpu_standard

# è¿è¡Œè®­ç»ƒ
./train_h200_8gpu_standard.sh
```

#### **æ–¹æ³•2: ç›´æ¥ä½¿ç”¨torchrun**
```bash
# H200 8-GPUæ ‡å‡†è®­ç»ƒ (é»˜è®¤500Kæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v3.py \
    --dataset hf \
    --batch-size 32 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 500k

# æŒ‡å®šæ•°æ®å¤§å° (5Mæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v3.py \
    --dataset hf \
    --batch-size 32 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 5M
```

#### **æ–¹æ³•3: ä½¿ç”¨é€šç”¨è„šæœ¬**
```bash
# ä½¿ç”¨é»˜è®¤H200 8-GPUè®¾ç½®
./run_multi_gpu_training.sh

# æˆ–è‡ªå®šä¹‰å‚æ•°
./run_multi_gpu_training.sh hf 32 5 3e-5 true
```

**ç‰¹ç‚¹:**
- æ”¯æŒå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
- å¤§æ¨¡å‹: 6å±‚, 8å¤´, 16ç»´åµŒå…¥
- æ··åˆç²¾åº¦è®­ç»ƒ
- è‡ªåŠ¨GPUæ£€æµ‹å’Œé…ç½®
- è®­ç»ƒæ—¶é—´: ~30åˆ†é’Ÿ (8GPU)

### ğŸš€ **v4: 24GBå†…å­˜ä¼˜åŒ–è®­ç»ƒ (H200/A10å…¼å®¹)**

#### **æ–¹æ³•1: ä½¿ç”¨é¢„é…ç½®è„šæœ¬ (æ¨è)**
```bash
# è¿è¡Œ24GBå†…å­˜ä¼˜åŒ–è®­ç»ƒ
./train_h200_8gpu_v4.sh
```

#### **æ–¹æ³•2: ç›´æ¥ä½¿ç”¨torchrun**
```bash
# 24GBå†…å­˜ä¼˜åŒ–è®­ç»ƒ (10Mæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 10M

# è‡ªå®šä¹‰æ•°æ®å¤§å° (5Mæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 5M
```

#### **æ–¹æ³•3: å•GPUè®­ç»ƒ (é€‚åˆA10)**
```bash
# å•GPU 24GBä¼˜åŒ–è®­ç»ƒ
python mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 10M
```

**ç‰¹ç‚¹:**
- **ä¸“é—¨é’ˆå¯¹24GB+ GPUä¼˜åŒ–** (H200, A10, RTX 4090ç­‰)
- **å¤§æ¨¡å‹é…ç½®**: 8å±‚, 8å¤´, 256ç»´åµŒå…¥
- **é«˜å†…å­˜åˆ©ç”¨ç‡**: 83% (20GB/24GB)
- **å¤§æ‰¹æ¬¡è®­ç»ƒ**: 96 per GPU (æ€»768)
- **é•¿åºåˆ—æ”¯æŒ**: 256 tokens
- **å¤§è¯æ±‡è¡¨**: 25Kè¯æ±‡
- **å¿«é€Ÿè®­ç»ƒ**: ~20åˆ†é’Ÿ (10Mæ ·æœ¬)
- **æ··åˆç²¾åº¦è®­ç»ƒ**: bfloat16ä¼˜åŒ–
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPU
- **è‡ªåŠ¨ç¼“å­˜**: æ™ºèƒ½æ•°æ®ç¼“å­˜ç³»ç»Ÿ

**é€‚ç”¨åœºæ™¯:**
- 24GB+ GPUç¯å¢ƒ (H200, A10, RTX 4090ç­‰)
- é«˜å†…å­˜åˆ©ç”¨ç‡éœ€æ±‚
- å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- éœ€è¦å¿«é€Ÿè®­ç»ƒçš„å¤§æ•°æ®é›†

**æ€§èƒ½ä¼˜åŠ¿:**
- **å†…å­˜åˆ©ç”¨ç‡**: ä»12%æå‡åˆ°83%
- **æ¨¡å‹å¤æ‚åº¦**: 150å€å¢åŠ  (ä»100Kåˆ°15Må‚æ•°)
- **è®­ç»ƒæ•ˆç‡**: æ˜¾è‘—æå‡
- **æ•°æ®åå**: 10å€å¢åŠ 
- **åºåˆ—é•¿åº¦**: 2å€å¢åŠ  (128â†’256)
- **è¯æ±‡è¡¨**: 2.5å€å¢åŠ  (10Kâ†’25K)

### 3. Test Streaming Functionality
```bash
python test_streaming.py
```

### 4. Test Caching Functionality
```bash
python test_cache.py
```

### 5. Manage Cache
```bash
# View cache information
python cache_manager.py info

# Clear cache
python cache_manager.py clear

# Show disk usage
python cache_manager.py usage
```

## ğŸ¯ **è¯¦ç»†è¿è¡ŒæŒ‡å—**

### **v1: å¿«é€Ÿå…¥é—¨è®­ç»ƒ**

**é€‚ç”¨åœºæ™¯**: å­¦ä¹ ã€æµ‹è¯•ã€å¿«é€ŸéªŒè¯

```bash
# åŸºæœ¬è¿è¡Œ
python mlm_pretrain_v1.py

# æŸ¥çœ‹å¸®åŠ©
python mlm_pretrain_v1.py --help
```

**è¾“å‡ºç¤ºä¾‹:**
```
Using device: cuda
Loading IMDB dataset for MLM pre-training...
Training samples: 22500
Validation samples: 2500
Vocabulary size: 10005
Starting MLM pre-training...
Epoch 1/3: Train Loss: 9.3330 | Val Loss: 9.2017
Epoch 2/3: Train Loss: 9.1415 | Val Loss: 9.0840
Epoch 3/3: Train Loss: 9.0580 | Val Loss: 9.0374
MLM pre-training completed!
```

### **v2: æ ‡å‡†è®­ç»ƒ**

**é€‚ç”¨åœºæ™¯**: æ ‡å‡†è®­ç»ƒã€ä¸­ç­‰è§„æ¨¡æ•°æ®é›†

```bash
# ä½¿ç”¨Hugging Faceæ•°æ®é›† (æµå¼æ¨¡å¼ï¼Œé»˜è®¤500Kæ ·æœ¬)
python mlm_pretrain_v2.py hf

# æŒ‡å®šæ•°æ®å¤§å° (5Mæ ·æœ¬ï¼Œæµå¼æ¨¡å¼)
python mlm_pretrain_v2.py hf true 5M

# æŒ‡å®šæ•°æ®å¤§å° (50Mæ ·æœ¬ï¼Œæœ¬åœ°ä¸‹è½½æ¨¡å¼)
python mlm_pretrain_v2.py hf false 50M

# ä½¿ç”¨IMDBæ•°æ®é›†
python mlm_pretrain_v2.py imdb

# æŸ¥çœ‹å¸®åŠ©
python mlm_pretrain_v2.py --help
```

**è¾“å‡ºç¤ºä¾‹:**
```
Using device: cuda
Loading dataset for MLM pre-training (choice: hf, streaming: True)...
Using larger model configuration for Hugging Face dataset...
Model configuration:
  - n_heads: 4
  - n_embed: 8
  - n_layers: 4
  - head_size: 2
  - num_epochs: 5
  - learning_rate: 3e-05
Total model parameters: 84,640
Starting MLM pre-training...
Epoch 1/5: Train Loss: 7.9531 | Val Loss: 6.6964
Epoch 2/5: Train Loss: 6.6408 | Val Loss: 6.5902
...
```

### **v3: å¤šGPUè®­ç»ƒ**

**é€‚ç”¨åœºæ™¯**: å¤§è§„æ¨¡è®­ç»ƒã€å¤šGPUç¯å¢ƒ

#### **æ­¥éª¤1: æŸ¥çœ‹å¯ç”¨é…ç½®**
```bash
python multi_gpu_configs.py list
```

#### **æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒè„šæœ¬**
```bash
# ç”ŸæˆH200 8-GPUæ ‡å‡†è®­ç»ƒè„šæœ¬
python multi_gpu_configs.py generate h200_8gpu_standard

# ç”ŸæˆH200 8-GPUå¿«é€Ÿè®­ç»ƒè„šæœ¬
python multi_gpu_configs.py generate h200_8gpu_fast

# ç”ŸæˆH200 8-GPUé«˜è´¨é‡è®­ç»ƒè„šæœ¬
python multi_gpu_configs.py generate h200_8gpu_quality
```

#### **æ­¥éª¤3: è¿è¡Œè®­ç»ƒ**
```bash
# è¿è¡Œç”Ÿæˆçš„è„šæœ¬
./train_h200_8gpu_standard.sh

# æˆ–ç›´æ¥ä½¿ç”¨torchrun (é»˜è®¤500Kæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v3.py \
    --dataset hf \
    --batch-size 32 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 500k

# æŒ‡å®šæ•°æ®å¤§å° (5Mæ ·æœ¬)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v3.py \
    --dataset hf \
    --batch-size 32 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 5M
```

**è¾“å‡ºç¤ºä¾‹:**
```
Multi-GPU MLM Training Setup:
  - World Size: 8
  - Local Rank: 0
  - Device: cuda:0
  - Dataset: hf
  - Streaming: true
  - Batch Size per GPU: 32
  - Total Batch Size: 256
  - Epochs: 5
  - Learning Rate: 3e-05
Using larger model configuration for Hugging Face dataset...
Model configuration:
  - n_heads: 8
  - n_embed: 16
  - n_layers: 6
  - head_size: 2
  - num_epochs: 5
  - learning_rate: 3e-05
Total model parameters: 182,112
Starting MLM pre-training...
Epoch 1/5: Train Loss: 6.1234 | Val Loss: 5.9876
...
```

### **v4: 24GBå†…å­˜ä¼˜åŒ–è®­ç»ƒ**

**é€‚ç”¨åœºæ™¯**: 24GB+ GPUç¯å¢ƒã€é«˜å†…å­˜åˆ©ç”¨ç‡éœ€æ±‚ã€å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ

#### **æ­¥éª¤1: æ£€æŸ¥GPUé…ç½®**
```bash
# æ£€æŸ¥GPUå†…å­˜
nvidia-smi

# ç¡®ä¿GPUå†…å­˜ >= 24GB
# æ”¯æŒçš„GPU: H200, A10, RTX 4090ç­‰
```

#### **æ­¥éª¤2: è¿è¡Œè®­ç»ƒ**
```bash
# æ–¹æ³•1: ä½¿ç”¨é¢„é…ç½®è„šæœ¬ (æ¨è)
./train_h200_8gpu_v4.sh

# æ–¹æ³•2: ç›´æ¥ä½¿ç”¨torchrun (8GPU)
torchrun \
    --nproc_per_node=8 \
    --nnodes=1 \
    --node_rank=0 \
    --master_addr=localhost \
    --master_port=12355 \
    mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 10M

# æ–¹æ³•3: å•GPUè®­ç»ƒ (é€‚åˆA10)
python mlm_pretrain_v4.py \
    --dataset hf \
    --batch-size 96 \
    --epochs 5 \
    --lr 3e-5 \
    --streaming true \
    --max-samples 10M
```

#### **æ­¥éª¤3: ç›‘æ§è®­ç»ƒ**
```bash
# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/v4_training_*.log
```

**è¾“å‡ºç¤ºä¾‹:**
```
Multi-GPU MLM Training v4 Setup (24GB Memory Optimized):
  - World Size: 8
  - Local Rank: 0
  - Device: cuda:0
  - Dataset: hf
  - Streaming: true
  - Batch Size per GPU: 96
  - Total Batch Size: 768
  - Epochs: 5
  - Learning Rate: 3e-05
  - Max Samples: 10000000
Using medium model configuration for Hugging Face dataset (optimized for 24GB GPU memory)...
Model configuration (v4 - 24GB optimized):
  - n_heads: 8
  - n_embed: 256
  - n_layers: 8
  - head_size: 32
  - num_epochs: 5
  - learning_rate: 3e-05
Total model parameters: 15,123,456
Starting MLM pre-training v4 (24GB optimized)...
Epoch 1/5: Train Loss: 5.2341 | Val Loss: 5.1234
...
```

**ç‰¹ç‚¹è¯´æ˜:**
- **é«˜å†…å­˜åˆ©ç”¨ç‡**: 83% (20GB/24GB)
- **å¤§æ¨¡å‹é…ç½®**: 8å±‚/8å¤´/256ç»´åµŒå…¥
- **å¤§æ‰¹æ¬¡è®­ç»ƒ**: 96 per GPU (æ€»768)
- **é•¿åºåˆ—æ”¯æŒ**: 256 tokens
- **å¤§è¯æ±‡è¡¨**: 25Kè¯æ±‡
- **å¿«é€Ÿè®­ç»ƒ**: ~20åˆ†é’Ÿ (10Mæ ·æœ¬)
- **æ··åˆç²¾åº¦**: bfloat16ä¼˜åŒ–
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒå¤šGPU
- **è‡ªåŠ¨ç¼“å­˜**: æ™ºèƒ½æ•°æ®ç¼“å­˜ç³»ç»Ÿ

## Model Configurations

ç³»ç»Ÿæ ¹æ®æ•°æ®é›†è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹é…ç½®ï¼š

### ğŸ¯ **v1é…ç½® (mlm_pretrain_v1.py)**
- **å±‚æ•°**: 2
- **æ³¨æ„åŠ›å¤´**: 2
- **åµŒå…¥ç»´åº¦**: 4
- **æœ€å¤§åºåˆ—é•¿åº¦**: 128
- **è¯æ±‡è¡¨å¤§å°**: 10,000
- **å‚æ•°æ•°é‡**: ~41K
- **è®­ç»ƒæ—¶é—´**: ~5åˆ†é’Ÿ
- **é€‚ç”¨åœºæ™¯**: å­¦ä¹ ã€æµ‹è¯•ã€å°æ•°æ®é›†

### ğŸš€ **v2é…ç½® (mlm_pretrain_v2.py)**
- **å±‚æ•°**: 4 (HF) / 2 (IMDB)
- **æ³¨æ„åŠ›å¤´**: 4 (HF) / 2 (IMDB)
- **åµŒå…¥ç»´åº¦**: 8 (HF) / 4 (IMDB)
- **æœ€å¤§åºåˆ—é•¿åº¦**: 128
- **è¯æ±‡è¡¨å¤§å°**: 10,000
- **å‚æ•°æ•°é‡**: ~84K (HF) / ~41K (IMDB)
- **è®­ç»ƒæ—¶é—´**: ~30åˆ†é’Ÿ (HF) / ~5åˆ†é’Ÿ (IMDB)
- **é€‚ç”¨åœºæ™¯**: å¤§æ•°æ®é›†ã€æ›´å¥½æ€§èƒ½

### âš¡ **v3é…ç½® (mlm_pretrain_v3.py)**
- **å±‚æ•°**: 6
- **æ³¨æ„åŠ›å¤´**: 8
- **åµŒå…¥ç»´åº¦**: 16
- **æœ€å¤§åºåˆ—é•¿åº¦**: 128
- **è¯æ±‡è¡¨å¤§å°**: 10,000
- **å‚æ•°æ•°é‡**: ~182K
- **è®­ç»ƒæ—¶é—´**: ~30åˆ†é’Ÿ (8GPU)
- **é€‚ç”¨åœºæ™¯**: å¤§è§„æ¨¡è®­ç»ƒã€å¤šGPUç¯å¢ƒ

### ğŸš€ **v4é…ç½® (mlm_pretrain_v4.py)**
- **å±‚æ•°**: 8
- **æ³¨æ„åŠ›å¤´**: 8
- **åµŒå…¥ç»´åº¦**: 256
- **æœ€å¤§åºåˆ—é•¿åº¦**: 256
- **è¯æ±‡è¡¨å¤§å°**: 25,000
- **å‚æ•°æ•°é‡**: ~15M
- **è®­ç»ƒæ—¶é—´**: ~20åˆ†é’Ÿ (8GPU)
- **é€‚ç”¨åœºæ™¯**: 24GB+ GPUç¯å¢ƒã€é«˜å†…å­˜åˆ©ç”¨ç‡éœ€æ±‚
- **å†…å­˜ä½¿ç”¨**: ~20GB/24GB (83%åˆ©ç”¨ç‡)
- **æ‰¹æ¬¡å¤§å°**: 96 per GPU (æ€»768)
- **æ··åˆç²¾åº¦**: bfloat16ä¼˜åŒ–

### ğŸ“Š **æ‰€æœ‰å¯ç”¨é…ç½®**
è¿è¡Œ `python model_config_comparison.py` æŸ¥çœ‹æ‰€æœ‰é…ç½®ï¼š

| é…ç½® | å±‚æ•° | å¤´æ•° | åµŒå…¥ | å‚æ•° | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|------|----------|
| **IMDB Small** | 2 | 2 | 4 | ~41K | IMDBæ•°æ®é›† |
| **HF Medium** | 4 | 4 | 8 | ~84K | HFæ•°æ®é›† |
| **HF Large** | 6 | 8 | 16 | ~182K | å¤§æ•°æ®é›† |
| **HF Extra Large** | 8 | 8 | 256 | ~15M | 24GB+ GPUä¼˜åŒ– |

## Dataset Options

### IMDB Dataset
- **Size**: ~25K samples
- **Domain**: Movie reviews
- **Pros**: Fast, focused domain
- **Cons**: Limited diversity

### Hugging Face Datasets
- **wikitext-103-raw-v1**: Wikipedia articles (1.8M tokens)
- **wikipedia**: Wikipedia articles (20220301.en)
- **openwebtext**: Web text (8M documents)
- **c4**: Common Crawl data (English)
- **pile-cc**: Common Crawl data (large)

## Caching System

The project includes an intelligent caching system:

- **Streaming Mode**: Downloads data on-the-fly and caches processed results
- **Cache Location**: `.dataset_cache/` directory
- **Cache Keys**: Based on dataset name, parameters, and configuration
- **Benefits**: 
  - First run: Downloads and processes data
  - Subsequent runs: Instant loading from cache
  - Disk usage: ~100-500MB vs ~1-10GB for local download

## Training Output

### Model Files
- `mlm_model.pth`: Full MLM model weights
- `microbert_model.pth`: Base MicroBERT model weights
- `tokenizer_vocab.json`: Vocabulary mapping
- `mlm_training_history.json`: Training metrics

### Visualization
- `training_history.png`: Loss curves over epochs

### Example Output
```
=== Testing MLM Model ===
1. Original: this movie is [MASK] fantastic
   [MASK] at position 3:
     that: logit=1.642, prob=0.216190
     ok,: logit=1.565, prob=0.200183
     disney: logit=1.559, prob=0.198995
     episode: logit=1.526, prob=0.192561
     can't: logit=1.524, prob=0.192071
```

## Performance Tips

### For Limited Resources
- Use IMDB dataset (`mlm_pretrain_v1.py`)
- Use streaming mode for HF datasets
- Reduce `max_samples` parameter

### For Better Results
- Use larger datasets (HF datasets)
- Increase training epochs
- Use local download mode for faster training

### For Development
- Use smaller `max_samples` for quick testing
- Monitor cache usage with `cache_manager.py`

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size per GPU
   - Use smaller model configuration
   - Use CPU training
   - Enable gradient checkpointing

2. **Dataset Loading Failures**
   - Check internet connection
   - Try different dataset
   - Use IMDB fallback
   - Check disk space for caching

3. **Cache Issues**
   - Clear cache: `python cache_manager.py clear`
   - Check disk space
   - Use different cache directory

4. **Multi-GPU Issues (v3/v4)**
   - Check GPU availability: `nvidia-smi`
   - Ensure NCCL is installed: `python -c "import torch; print(torch.cuda.nccl.version())"`
   - Check port conflicts: change `--master_port=12356`
   - Verify PyTorch installation: `python -c "import torch; print(torch.cuda.device_count())"`

5. **v4 Memory Issues**
   - Ensure GPU memory >= 24GB for v4
   - Reduce batch size if memory insufficient: `--batch-size 64`
   - Use smaller model: switch to v3 if needed
   - Check memory usage: `nvidia-smi`

6. **Distributed Training Issues**
   - Check if all GPUs are visible
   - Ensure proper environment variables are set
   - Try single GPU first: `python mlm_pretrain_v3.py --dataset imdb` or `python mlm_pretrain_v4.py --dataset imdb`

### Getting Help

- Check the training logs for error messages
- Verify all dependencies are installed
- Ensure sufficient disk space for caching
- For multi-GPU issues, check `MULTI_GPU_USAGE.md`
- Test single GPU functionality before multi-GPU training

## ğŸ“Š **ç‰ˆæœ¬å¯¹æ¯”æ€»ç»“**

| ç‰¹æ€§ | v1 (å¿«é€Ÿå…¥é—¨) | v2 (æ ‡å‡†è®­ç»ƒ) | v3 (å¤šGPUè®­ç»ƒ) | v4 (24GBä¼˜åŒ–) |
|------|---------------|---------------|----------------|---------------|
| **é€‚ç”¨åœºæ™¯** | å­¦ä¹ ã€æµ‹è¯• | æ ‡å‡†è®­ç»ƒ | å¤§è§„æ¨¡è®­ç»ƒ | 24GB+ GPUä¼˜åŒ– |
| **æ•°æ®é›†** | IMDB | IMDB + HF | IMDB + HF | IMDB + HF |
| **æ¨¡å‹å¤§å°** | å° (41Kå‚æ•°) | ä¸­ (84Kå‚æ•°) | å¤§ (182Kå‚æ•°) | è¶…å¤§ (15Må‚æ•°) |
| **è®­ç»ƒæ—¶é—´** | ~5åˆ†é’Ÿ | ~30åˆ†é’Ÿ | ~30åˆ†é’Ÿ (8GPU) | ~20åˆ†é’Ÿ (8GPU) |
| **GPUéœ€æ±‚** | 1ä¸ª | 1ä¸ª | å¤šä¸ª | å¤šä¸ª |
| **å†…å­˜éœ€æ±‚** | ä½ | ä¸­ç­‰ | é«˜ | å¾ˆé«˜ (24GB+) |
| **æµå¼å¤„ç†** | âŒ | âœ… | âœ… | âœ… |
| **ç¼“å­˜ç³»ç»Ÿ** | âŒ | âœ… | âœ… | âœ… |
| **æ··åˆç²¾åº¦** | âŒ | âŒ | âœ… | âœ… |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | âŒ | âŒ | âœ… | âœ… |
| **å†…å­˜åˆ©ç”¨ç‡** | ä½ | ä¸­ç­‰ | ä¸­ç­‰ | é«˜ (83%) |
| **æ‰¹æ¬¡å¤§å°** | 32 | 32 | 32 per GPU | 96 per GPU |
| **åºåˆ—é•¿åº¦** | 128 | 128 | 128 | 256 |
| **è¯æ±‡è¡¨å¤§å°** | 10K | 10K | 10K | 25K |

## ğŸ¯ **é€‰æ‹©å»ºè®®**

- **åˆå­¦è€…/æµ‹è¯•**: ä½¿ç”¨ `v1` - å¿«é€Ÿã€ç®€å•ã€èµ„æºéœ€æ±‚ä½
- **æ ‡å‡†è®­ç»ƒ**: ä½¿ç”¨ `v2` - å¹³è¡¡æ€§èƒ½å’Œèµ„æºéœ€æ±‚
- **å¤§è§„æ¨¡è®­ç»ƒ**: ä½¿ç”¨ `v3` - å……åˆ†åˆ©ç”¨å¤šGPUèµ„æº
- **24GB+ GPUç¯å¢ƒ**: ä½¿ç”¨ `v4` - é«˜å†…å­˜åˆ©ç”¨ç‡ã€å¤§æ¨¡å‹ã€å¿«é€Ÿè®­ç»ƒ

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Based on the BERT architecture from "Attention Is All You Need"
- Uses Hugging Face datasets and transformers libraries
- Inspired by educational implementations of transformer models 